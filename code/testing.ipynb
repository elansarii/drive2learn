{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0541cf74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: 90%"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import gymnasium as gym\n",
    "import highway_env\n",
    "import highway_env.envs\n",
    "import highway_env.envs.common\n",
    "import highway_env.envs.common.observation\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "class HighwayAgent:\n",
    "    def __init__(self, method, env, alpha = 0.1, gamma = 0.9, epsilon_start = 0.8, epsilon_decay = 0.9999, epsilon_min = 0.1):\n",
    "        self.method = method\n",
    "        self.env = env\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon_start = self.epsilon = epsilon_start\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        \n",
    "        self.q_values = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    \n",
    "    def getAction(self, state, isOptimal = False):  # isOptimal = True, in case you want completely greedy (usually used in testing)\n",
    "        if np.random.rand() > self.epsilon or isOptimal:\n",
    "            return np.argmax(self.q_values[state])\n",
    "        else:\n",
    "            return self.env.action_space.sample()\n",
    "            \n",
    "        \n",
    "    def update(self, state, action, reward, next_state, next_action, done):\n",
    "        if self.method == 'SARSA':\n",
    "            self.q_values[state][action] += self.alpha * (reward + self.gamma * self.q_values[next_state][next_action] * (not done) - self.q_values[state][action]) \n",
    "\n",
    "        elif self.method == 'Expected SARSA':\n",
    "            action_prob = np.ones(self.env.action_space.n) * (self.epsilon / self.env.action_space.n)\n",
    "            best_action = self.getAction(next_state, True)\n",
    "            action_prob[best_action] += (1 - self.epsilon)\n",
    "            expected =  np.dot(action_prob, self.q_values[next_state])\n",
    "            self.q_values[state][action] += self.alpha * (reward + self.gamma * expected * (not done) - self.q_values[state][action])\n",
    "        \n",
    "        elif self.method == 'Q':\n",
    "            self.q_values[state, action] += self.alpha * (reward + self.gamma * np.max(self.q_values[next_state]) * (not done) - self.q_values[state, action])\n",
    "\n",
    "        elif self.method == 'Double Q':\n",
    "            if np.random.rand() > 0.5:\n",
    "                next_action = np.argmax(self.q_values[next_state])\n",
    "                self.q_values[state, action] += self.alpha * (reward + self.gamma * self.q_values2[next_state, next_action] * (not done) - self.q_values[state, action])\n",
    "            else:\n",
    "                next_action = np.argmax(self.q_values2[next_state]) \n",
    "    \n",
    "    def decayEpsilon(self, episode):\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon_start * (self.epsilon_decay ** episode))\n",
    "\n",
    "class CustomHighwayObs(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.ego = env.unwrapped.vehicle\n",
    "\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "            obs, info = self.env.reset(**kwargs)\n",
    "            modified_obs = tuple(self.cont2discrete(obs))\n",
    "            return modified_obs, info\n",
    "\n",
    "    def step(self, action):\n",
    "        # Original environment step function, obs is kinematics and absoulte\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "        modified_obs = tuple(self.cont2discrete(obs))\n",
    "\n",
    "        return modified_obs, reward, terminated, truncated, info\n",
    "    \n",
    "    def _calcCar(self, car_obs):\n",
    "        if car_obs[0] > 0:\n",
    "                distance = car_obs[1]  # Distance between the car and us, (-) means we are infront\n",
    "                speed = car_obs[3] # Relative speed between us and car, (-) means we are slower\n",
    "                # print(speed)\n",
    "\n",
    "                # Calculating ttc, if ttc is negative it means ttc is effectavily inf\n",
    "                if speed <= 1e-6 or distance <= 0:\n",
    "                    ttc = -1  # Handle invalid TTC\n",
    "                else:\n",
    "                    ttc = distance / speed\n",
    "\n",
    "                if ttc < 0 or ttc > 6:\n",
    "                    ttc_bin = 3\n",
    "                elif ttc < 2:\n",
    "                    ttc_bin = 0\n",
    "                elif ttc < 4:\n",
    "                    ttc_bin = 1\n",
    "                else:  # 4 <= ttc <= 6\n",
    "                    ttc_bin = 2\n",
    "\n",
    "                # Getting the lane\n",
    "                y_pos = round(car_obs[2], 2)\n",
    "                if abs(y_pos) < 0.04 / 2:  # Small tolerance for floating-point precision\n",
    "                    lane_index = 0  # Same lane as ego vehicle\n",
    "                \n",
    "                # Use rounding to determine the lane index\n",
    "                lane_index = np.clip(round(y_pos / 0.04), -2, 2)\n",
    "                # 0 same lane, 1 immediate right, 2 Far right etc\n",
    "                \n",
    "        else:\n",
    "            # Default values if car isnt on screen\n",
    "            ttc_bin = 3\n",
    "            lane_index = 0\n",
    "        return (ttc_bin, lane_index)\n",
    "    \n",
    "    def cont2discrete(self, obs):\n",
    "        '''\n",
    "        This function gets the old kinematic observation and returns our new discrete observation [ttc_bin, lane, heading]\n",
    "        where:\n",
    "        ttc_bin: a bin that represents ttc disrubuated as such {0: 0-2, 1: 2-4, 2:4-6, 3: >6}\n",
    "        lane: represents which lane said vehicle is\n",
    "        heading: represents where that vehicle is going (1 is down, -1 is up, 0 is straight)\n",
    "                                                0        1  2   3   4\n",
    "        NOTE: Old obs is as such for each car [Presance, x, y, vx, vy]\n",
    "        For y, each lane occupies a 0.25 space, from 0 to 1\n",
    "        For vy, positve is going down, negative is going up\n",
    "        '''\n",
    "        temp = obs[1:]\n",
    "        new_obs = []\n",
    "        ego_obs = obs[0]\n",
    "        for index, car_obs in enumerate(temp):\n",
    "            if car_obs[0] > 0:\n",
    "                distance = car_obs[1]  # Distance between the car and us, (-) means we are infront\n",
    "                speed = car_obs[3] # Relative speed between us and car, (-) means we are slower\n",
    "                # print(speed)\n",
    "\n",
    "                # Calculating ttc, if ttc is negative it means ttc is effectavily inf\n",
    "                if speed <= 1e-6 or distance <= 0:\n",
    "                    ttc = -1  # Handle invalid TTC\n",
    "                else:\n",
    "                    ttc = distance / speed\n",
    "\n",
    "                if ttc < 0 or ttc > 6:\n",
    "                    ttc_bin = 3\n",
    "                elif ttc < 2:\n",
    "                    ttc_bin = 0\n",
    "                elif ttc < 4:\n",
    "                    ttc_bin = 1\n",
    "                else:  # 4 <= ttc <= 6\n",
    "                    ttc_bin = 2\n",
    "\n",
    "                # Getting the lane\n",
    "                y_pos = round(car_obs[2], 2)\n",
    "                if abs(y_pos) < 0.04 / 2:  # Small tolerance for floating-point precision\n",
    "                    lane_index = 0  # Same lane as ego vehicle\n",
    "                \n",
    "                # Use rounding to determine the lane index\n",
    "                lane_index = np.clip(round(y_pos / 0.04), -2, 2)\n",
    "                # 0 same lane, 1 immediate right, 2 Far right etc\n",
    "                \n",
    "            else:\n",
    "                # Default values if car isnt on screen\n",
    "                ttc_bin = 3\n",
    "                lane_index = 0\n",
    "\n",
    "            new_obs.append((ttc_bin, lane_index))\n",
    "        return new_obs\n",
    "    \n",
    "def train(method, num_episodes = 40_000, alpha = 0.1, gamma = 0.9, epsilon_start = 0.8, epsilon_decay = 0.9999, epsilon_min = 0.1, showProgress = False):             \n",
    "    env = gym.make(\n",
    "        'highway-fast-v0', \n",
    "        config = {\n",
    "        \"observation\": {\n",
    "            \"type\": \"Kinematics\",\n",
    "            \"features\": [\"presence\", \"x\", \"y\", \"vx\", \"heading\"],\n",
    "            \"features_range\": {\n",
    "                \"x\": [-100, 100],\n",
    "                \"y\": [-100, 100],\n",
    "                \"vx\": [-20, 20],\n",
    "                \"heading\": [-22/7, 22/7]\n",
    "            },\n",
    "            \"absolute\": False,\n",
    "        },\n",
    "        \"vehicles_count\": 10,\n",
    "\n",
    "    }\n",
    "    )\n",
    "\n",
    "    env = CustomHighwayObs(env)\n",
    "\n",
    "    agent = HighwayAgent(method, env, alpha, gamma, epsilon_start, epsilon_decay, epsilon_min)\n",
    "    max_steps = 10\n",
    "    episode_rewards = []\n",
    "    smoothed_rewards = []\n",
    "    window = 1000\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()[0]\n",
    "        action = agent.getAction(state)\n",
    "        total_reward = steps = 0\n",
    "        done = False\n",
    "\n",
    "        # Episode Start\n",
    "        while not done and steps <= max_steps:\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            next_action = agent.getAction(next_state)\n",
    "            done = terminated or truncated\n",
    "            total_reward += reward\n",
    "\n",
    "            agent.update(state, action, reward, next_state, next_action, done)\n",
    "\n",
    "            action = next_action\n",
    "            state =  next_state\n",
    "            steps += 1\n",
    "        # Episode end\n",
    "        \n",
    "        episode_rewards.append(total_reward)\n",
    "        agent.decayEpsilon(episode)\n",
    "        smoothed_rewards.append(np.mean(episode_rewards[-window:]))\n",
    "        if showProgress and episode % (num_episodes/10) == 0:\n",
    "            print(f'\\r{method}: {int(episode/num_episodes*100)}%', end='', flush=True)\n",
    "    env.close()\n",
    "    return smoothed_rewards, env, agent\n",
    "\n",
    "smoothed,  env, agent = train(\"Q\", num_episodes=3_000, showProgress=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "845cc6ba",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'get_image'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 27\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1000\u001b[39m):\n\u001b[0;32m     26\u001b[0m     action \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mgetAction(state, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 27\u001b[0m     state, reward, done, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m     env\u001b[38;5;241m.\u001b[39mrender()\n\u001b[0;32m     29\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(env\u001b[38;5;241m.\u001b[39mrender())\n",
      "Cell \u001b[1;32mIn[15], line 66\u001b[0m, in \u001b[0;36mCustomHighwayObs.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;66;03m# Original environment step function, obs is kinematics and absoulte\u001b[39;00m\n\u001b[1;32m---> 66\u001b[0m     obs, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m     modified_obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcont2discrete(obs))\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m modified_obs, reward, terminated, truncated, info\n",
      "File \u001b[1;32mc:\\Users\\ali23\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gymnasium\\wrappers\\common.py:393\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[0;32m    392\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 393\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ali23\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gymnasium\\core.py:327\u001b[0m, in \u001b[0;36mWrapper.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    323\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\n\u001b[0;32m    324\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: WrapperActType\n\u001b[0;32m    325\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[0;32m    326\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 327\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ali23\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gymnasium\\wrappers\\common.py:285\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ali23\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\highway_env\\envs\\common\\abstract.py:240\u001b[0m, in \u001b[0;36mAbstractEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    235\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[0;32m    236\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe road and vehicle must be initialized in the environment implementation\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    237\u001b[0m     )\n\u001b[0;32m    239\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpolicy_frequency\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m--> 240\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_simulate\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    242\u001b[0m obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation_type\u001b[38;5;241m.\u001b[39mobserve()\n\u001b[0;32m    243\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reward(action)\n",
      "File \u001b[1;32mc:\\Users\\ali23\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\highway_env\\envs\\common\\abstract.py:280\u001b[0m, in \u001b[0;36mAbstractEnv._simulate\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    275\u001b[0m     \u001b[38;5;66;03m# Automatically render intermediate simulation steps if a viewer has been launched\u001b[39;00m\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;66;03m# Ignored if the rendering is done offscreen\u001b[39;00m\n\u001b[0;32m    277\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    278\u001b[0m         frame \u001b[38;5;241m<\u001b[39m frames \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    279\u001b[0m     ):  \u001b[38;5;66;03m# Last frame will be rendered through env.render() as usual\u001b[39;00m\n\u001b[1;32m--> 280\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_automatic_rendering\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    282\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menable_auto_render \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ali23\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\highway_env\\envs\\common\\abstract.py:340\u001b[0m, in \u001b[0;36mAbstractEnv._automatic_rendering\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    338\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_record_video_wrapper\u001b[38;5;241m.\u001b[39mvideo_recorder\u001b[38;5;241m.\u001b[39mcapture_frame()\n\u001b[0;32m    339\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 340\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ali23\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\highway_env\\envs\\common\\abstract.py:308\u001b[0m, in \u001b[0;36mAbstractEnv.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    306\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mviewer\u001b[38;5;241m.\u001b[39mhandle_events()\n\u001b[0;32m    307\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb_array\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 308\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mviewer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_image\u001b[49m()\n\u001b[0;32m    309\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m image\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'get_image'"
     ]
    }
   ],
   "source": [
    "env = gym.make(\n",
    "        'highway-fast-v0', \n",
    "        render_mode='rgb_array', \n",
    "        config = {\n",
    "        \"observation\": {\n",
    "            \"type\": \"Kinematics\",\n",
    "            \"features\": [\"presence\", \"x\", \"y\", \"vx\", \"heading\"],\n",
    "            \"features_range\": {\n",
    "                \"x\": [-100, 100],\n",
    "                \"y\": [-100, 100],\n",
    "                \"vx\": [-20, 20],\n",
    "                \"heading\": [-22/7, 22/7]\n",
    "            },\n",
    "            \"absolute\": False,\n",
    "        },\n",
    "        \"vehicles_count\": 10,\n",
    "\n",
    "    }\n",
    "    )\n",
    "\n",
    "env = CustomHighwayObs(env)\n",
    "\n",
    "\n",
    "state = env.reset()[0]\n",
    "for _ in range(1000):\n",
    "    action = agent.getAction(state, True)\n",
    "    state, reward, done, truncated, info = env.step(action)\n",
    "    env.render()\n",
    "plt.imshow(env.render())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
